% !TEX TS-program = lualatex
\documentclass[a4paper, 11pt, fleqn, twoside]{scrreprt}
% escapeinside
%\usepackage[gray]{xcolor}
\usepackage[T1]{fontenc}        % T1-Fonts
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{proof} % 
%%http://www.logicmatters.net/resources/ndexamples/proofsty.html
%\usepackage{bussproofs}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{natbib}
\usepackage{abstract}
\usepackage[automark, headsepline]{scrlayer-scrpage}

% Kapitelüberschrift in der Kopfzeile
%\usepackage[automark]{scrpage2} % Schickerer Satzspiegel mit KOMA-Script
%\pagestyle{scrheadings}

% Minted
\usepackage{fontspec}

\setmonofont{Fira Code Regular}
\setmainfont{XCharter Roman}
\newfontfamily{\fallbackfont}{Source Code Pro}[Scale=MatchLowercase]
\DeclareTextFontCommand{\textfallback}{\fallbackfont}

\usepackage{newunicodechar}
\newunicodechar{∀}{\textfallback{∀}}
\newunicodechar{≔}{:=}

\usepackage{minted}
\usemintedstyle[Haskell]{trac, fontsize=\small}
\usemintedstyle[Coq]{default, encoding=utf8, fontsize=\small}
\usemintedstyle[Text]{default, fontsize=\small}
%framesep=10pt}
\setmintedinline{style = bw, fontsize=\small}

% Für schönere Tabellen (optional)
% \usepackage{booktabs}           % Netteres Tabellenlayout
% \usepackage{multicol}           % Mehrspaltige Bereiche
% \usepackage{tabularx}           % Tabellen mit fester Breite

% Für Listings
% \usepackage{listings}

% Eine kleine Hilfe für offene Lücken
\newcommand{\todo}[1]{\marginpar{\textbf{TODO:} #1}}
\newcommand{\hinl}[1]{\mintinline{Haskell}{#1}}
\newcommand{\cinl}[1]{\mintinline{Coq}{#1}}

\setlength\partopsep{-\topsep}
\addtolength\partopsep{-\parskip}
\addtolength\partopsep{0.32cm}

\begin{document}
\pagenumbering{roman} % römische Seitenzahlen

\begin{titlepage}
	\vspace*{3cm}
	\centering
	{\huge\bfseries Modelling Call-Time Choice as Effect using Scoped Free 
	Monads\par}
	\vspace{1cm}
	\textbf{Niels Bunkenburg} \par 
	\vspace{6cm}
	\textbf{Master's Thesis} \par
	Programming Languages and Compiler Construction \par
	Department of Computer Science \par
	Kiel University
	\vfill
	Advised by\par
	Priv.-Doz. Dr. Frank Huch \par
	M. Sc. Sandra Dylus
	\vfill
	% Randloses Drucken nicht möglich...
	\tikz[remember picture,overlay] \node[opacity=0.3,inner sep=0pt] at 
	(9.5,-1.5){\includegraphics{img/cau-siegel-1400.png}};
	{\large \today\par}
\end{titlepage}

\newpage
\thispagestyle{empty}
\strut
\newpage

\chapter*{Erklärung der Urheberschaft}
\vspace{2cm}
Ich erkläre hiermit an Eides statt, dass ich die vorliegende Arbeit ohne Hilfe Dritter und ohne Benutzung anderer als der angegebenen Hilfsmittel angefertigt habe.
Aus fremden Quellen direkt oder indirekt übernommene Gedanken sind als solche kenntlich gemacht.
Die Arbeit wurde bisher in gleicher oder ähnlicher Form in keiner anderen Prüfungsbehörde vorgelegt und auch noch nicht veröffentlicht.

\vspace{4cm}
\hspace{1cm} $\overline{~~~~~~~~~~\mbox{Ort, Datum}~~~~~~~~~~}$ \hfill $\overline{~~~~~~~~~~~~~\mbox{Unterschrift}~~~~~~~~~~~~~}$ \hspace{1cm}

\newpage
\mbox{}
\thispagestyle{empty}
\newpage

\begin{abstract}

\end{abstract}

\newpage
\strut
% This is necessary to apply the command that disables lexer errors for Unicode
\begin{minted}{Coq} 
\end{minted}

\begin{minted}{Haskell}
\end{minted}

% Disable highlighting of lexer errors for Unicode
\makeatletter
\expandafter\def\csname PYGdefault@tok@err\endcsname{\def\PYGdefault@bc##1{##1}}
\makeatother

\makeatletter
\expandafter\def\csname PYGtrac@tok@err\endcsname{\def\PYGtrac@bc##1{##1}}
\makeatother

\thispagestyle{empty}
\newpage

% Verzeichnisse
\renewcommand{\contentsname}{Contents}
\tableofcontents   % Inhaltsverzeichnis
%\listoffigures     % Abbildungsverzeichnis
% \listoftables      % Tabellenverzeichnis
% \lstlistoflistings % Abbildungsverzeichnis

\newpage               % Expliziter Umbruch für Seitenzahlen
\pagenumbering{arabic} % arabische Seitenzahlen

% Inhalt

\chapter{Introduction}

\chapter{Preliminaries}

\section{Coq}
\label{sec:coqIntro}
\begin{itemize}
\item Introduce the necessary Coq concepts to understand the paper
\end{itemize}

\section{Haskell}
\begin{itemize}
\item Introduce the necessary Haskell concepts to understand the paper
\end{itemize}
\subsection{Monad and MonadPlus}

\section{Curry}
\begin{itemize}
\item Introduce the necessary Curry concepts to understand the paper
\end{itemize}
\subsection{Non-strictness}
\subsection{Sharing}
\subsection{Non-determinism}

\section{Modelling Curry Programs using Monadic Code Transformation}
\begin{itemize}
\item Why is the naive MonadPlus approach not sufficient to model Curry semantic?
\item Motivate usage of monadic data types
\item Introduce explicit sharing
\end{itemize}

Modelling Curry programs in a language like Haskell requires a transformation of non-deterministic code into a semantically equivalent, deterministic program.
First, we have a look at the direct representation of non-determinism used in the KiCS2 implementation as described by \citet{brassel2011kics2}.

\subsection{KiCS2 Approach}
Non-determinism in Curry is not limited to \textit{flat} non-determinism but can occur within components of data structures and anywhere in a computation.
This means that expressing non-determinism via Haskell's list monad is not sufficient to model Curry's non-determinism.
\todo{Example}
Instead, existing data types receive additional constructors that represent failure and the choice between two values.
For example, the extended list data type looks as follows.

\begin{minted}{Haskell}
data List a = Nil | Cons a (List a) | Choice (List a) (List a) | Fail
\end{minted}

Since this transformation adds new constructors, all functions need to cover these cases, too.
The new rules return \hinl{Fail} if the function's argument is a failed computation and distribute function calls to both branches if the argument is a choice.

One issue with this approach is that call-time choice is not implemented yet.
If a choice is duplicated during evaluation, this information cannot be recovered later.
Therefore, each \hinl{Choice} constructor has an additional \hinl{ID} argument that identifies the same choices.
Since each choice needs a fresh ID, functions use an additional \hinl{IDSupply} argument when choices are created.

The evaluation of a non-deterministic value is implemented by transforming the value into a search tree which can be traversed with different search strategies.
In the process, each choice ID's decision is stored and then repeated if the same ID is encountered again.

While this approach is useful when the host language supports laziness and sharing, another approach is necessary to model these effects when they are not built into the language.

\subsection{Modelling Laziness and Sharing}
\citet{fischer2009purely} introduce a monadic representation of non-determinism that supports sharing and non-strict evaluation.
Out of simplicity, the implementation idea is presented in Haskell, similar to the approach of the original authors, using the example of permutation sort.
The algorithm consists  of three components.
Firstly, a function \hinl{insert} that inserts an element non-deterministically at every possible position within a list.

\begin{minted}{Haskell}
insert :: MonadPlus m => a -> [a] -> m [a]
insert x xs = return (x:xs)
      `mplus` case xs of
                []     -> mzero
                (y:ys) -> do zs <- insert x ys
                             return (y:zs)
\end{minted}

The second part is the function \hinl{perm} that inserts the head of a given list into the permutations of the list's tail.

\begin{minted}{Haskell}
perm :: MonadPlus m => [a] -> m [a]
perm [] = return []
perm (x:xs) = do ys <- perm xs
                 zs <- insert x ys
                 return zs
\end{minted}

Finally, the function \hinl{sort} generates permutations and then tests whether they are sorted.

\begin{minted}{Haskell}
sort :: MonadPlus m => [Int] -> m [Int]
sort xs = do ys <- perm xs
             guard (isSorted ys)
             return ys
\end{minted}

The function \hinl{isSorted} compares each element in a list to the next one to determine whether the list is sorted.
When we test this implementation, we can see that the runtime increases significantly when adding even a few elements.

\begin{minted}{Haskell}
λ> sort [9, 8..1] :: [[Int]]
[[1,2,3,4,5,6,7,8,9]]
(0.69 secs)
λ> sort [10, 9..1] :: [[Int]]
[[1,2,3,4,5,6,7,8,9,10]]
(6.67 secs)
λ> sort [11, 10..1] :: [[Int]]
[[1,2,3,4,5,6,7,8,9,10,11]]
(77.54 secs)
\end{minted}

The reason for the factorial runtime is that the implementations is needlessly strict.
A list of length $n$  has $n!$ permutations, all of which are generated when running \hinl{sort}.
This matches our observation above, since adding a tenth element increases the runtime by a factor of 10 and an eleventh element multiplies the runtime of the ten-element list by eleven.

If we consider the implementation of \hinl{isSorted}, we can see that, as soon as the comparison of two elements yields \hinl{False}, the function returns \hinl{False} and does not evaluate the remainder of the list.

\begin{minted}{Haskell}
isSorted :: [Int] -> Bool
isSorted (x:y:zs) = (x <= y) && isSorted (y:zs)
isSorted _        = True
\end{minted}

However, since we use bind to pass permutations from \hinl{perm} to \hinl{isSorted}, each permutation is fully evaluated before it is determined whether the permutation is sorted.
This leads to the complete evaluation of every permutation, which results in an inefficient program.

Similarly, when we consider the Curry example \hinl{head (1 : head [] : [])}, the strictness of our \hinl{MonadPlus} approach shows again.
The corresponding Haskell expression is as follows.

\mint{Haskell}{hd [] >>= \x -> hd (1 : x : [])}

Here \hinl{hd :: MonadPlus a => [a] -> m a} is the lifted \hinl{head} function.
Evaluating the expression in Haskell yields \hinl{mzero}, that is, no result, while Curry returns \hinl{1}.
The reason is the definition of the bind operator.
For example, the monad instance for lists defines bind as \hinl{xs >>= f = concatMap f xs}.
In the expression above, this means that the pattern matching within \hinl{concatMap} evaluates \hinl{hd []} to \hinl{mzero} and thus returns \hinl{mzero}.

The strictness observed in both examples is the motivation for an alternative approach.
The problem with the above implementations is that non-deterministic arguments of constructors need to be evaluated completely before the computation can continue.
Therefore, we would like to be able to use unevaluated, non-deterministic computations as arguments of constructors.

As mentioned before, we can implement this idea by adapting all data types so that they may contain non-deterministic components.

\begin{minted}{Haskell}
data List m a = Nil | Cons (m a) (m (List m a))
\end{minted}

The list data type now has an additional argument \hinl{m} of type \hinl{* -> *} that represents a non-determinism monad.
Instead of fixed constructors like \hinl{Choice}, the monad \hinl{m} determines the structure and evaluation strategy of the non-determinism effect.
Two smart constructors \hinl{cons} and \hinl{nil} make handling the new list type more convenient.

\begin{minted}{Haskell}
nil :: Monad m => m (List m a)
nil = return Nil

cons :: Monad m => m a -> m (List m a) -> m (List m a)
cons x y = return (Cons x y)
\end{minted}

Adapting the permutation sort functions to the lifted data type requires us to replace \hinl{[]} with \hinl{List m} 
However, this is not sufficient because the list itself can be the result of a non-deterministic computation.
Therefore, an additional \hinl{m} is wrapped around every occurrence of \hinl{List}.

\begin{minted}{Haskell}
insert' :: MonadPlus m => m a -> m (List m a) -> m (List m a)
insert' mx mxs = cons mx mxs
  `mplus` mxs >>= \xs -> case xs of
                           Nil         -> mzero
                           Cons my mys -> cons my (insert' mx mys)

perm' :: MonadPlus m => m (List m a) -> m (List m a)
perm' ml = ml >>= \l ->
  case l of
    Nil -> nil
    Cons mx mxs -> insert' mx (perm' mxs)
\end{minted}

Whenever pattern matching occurred in the original definition, we now use bind to extract a \hinl{List} value.
Since this only evaluates flat non-determinism and not non-determinism that occurs in the components, non-strictness is upheld as much as possible.

All functions now take arguments of the same type they return.
Thus, the definition of \hinl{sort} does not need bind in order to pass permutations to \hinl{isSorted}.

\begin{minted}{Haskell}
sort' :: MonadPlus m => m (List m Int) -> m (List m Int)
sort' xs = let ys = perm' xs in
  isSorted' ys >>= \sorted -> guard sorted >> ys
\end{minted}

We are now able to take advantage of \hinl{isSorted}'s non-strict definition.
The implementation generates permutations only if there is a chance that the permutation is sorted, that is, only recursive calls of \hinl{perm} that are demanded by \hinl{isSorted} are executed.

We reconsider the Curry example \hinl{head (1 : head [] : [])}.
Since the \hinl{List} data type now takes monad values as arguments, we can write the example using the smart constructors and a lifted \hinl{head} function as follows.

\begin{minted}{Haskell}
λ> hd' (cons (return 1) (cons (hd' nil) nil))
1
\end{minted}

Because we do not need to use bind to get the result of \hinl{hd' nil}, the expression is not evaluated due to non-strictness and the result is equal to Curry's output.

Data types with non-deterministic components solve the problem of non-strictness because each component can be evaluated individually, instead of forcing the evaluation of the whole term.
Unfortunately, this leads to a problem.
\label{sec:sharingComputations}
When unevaluated components are shared via Haskell's built-in sharing, computations, rather than results, are being shared.
This means that the results can be different each time the computation is evaluated, which contradicts the intuition of sharing.

The solution to this problem is an explicit sharing combinator \hinl{share :: m a -> m (m a)} that allows sharing the results of a computation in a non-strict way.
Here, \hinl{m} is a \hinl{MonadPlus} instance, similar to the monad used in the definition of the data type, that supports sharing.
Thus, \hinl{share} takes a  computation and then returns a computation that returns the result, that is, the shared value.
The reason for this nesting of monad layers is that, in short, the \hinl{share} combinator performs some actions that can be immediately executed by bind (the outer monad layer), while the inner monad layer should only be evaluated when needed.
This is explained in more detail later.
With the explicit sharing operator we can adapt \hinl{perm'} to share the generated permutations in order to achieve non-strictness in combination with sharing.

\begin{minted}{Haskell}
sort' :: MonadPlus m => m (List m Int) -> m (List m Int)
sort' xs = do ys <- share (perm' xs)
              sorted <- isSorted'
              guard sorted
              ys
\end{minted}

The \hinl{share} operator must satisfy certain laws, which we discuss in \autoref{sec:lawsOfSharing}.
The implementation of \hinl{share} is subject of the next chapter.

\chapter{Call-Time Choice modelled in Haskell}
Based on the ideas presented in the last chapter, we now want to model call-time choice, that is, non-strictness, sharing and non-determinism, in Haskell.
We still use \hinl{MonadPlus} to parameterize our programs.
However, instead of, for example, using the list instance to make non-determinism visible, we define an effect functor that can express many different effects, including non-determinism and sharing.
This approach, as introduced by \citet{wu2014effect}, will also be the base of the Coq implementation shown in \autoref{ch:callTimeChoiceCoq}.
\todo{Definition effect}

For the implementation of call-time choice, we want to be able to express different effects within our programs.
However, not every program contains effects.
There are also \textit{pure} programs that have no side-effects besides the computation of a value.
A data type that represents such programs could look as follows.

\begin{minted}{Haskell}
data Void a = Return a
\end{minted}

Here, \hinl{Void} means the absence of effects.

If we consider programs that contain effects, also called \textit{impure}, like, non-determinism, a data type that represents such values could look like the following.

\begin{minted}{Haskell}
data ND a = Return a
          | Fail
          | Choice (ND a) (ND a)
\end{minted}

This data type also has a constructor to model pure values but in addition, there are constructors that represent failed computations and the non-deterministic choice between two values.
We could go on and list data types that model many more effects but the question is: Is it possible to create a data type that, if appropriately instantiated, behaves like the original effect functor?
This would allow us to represent programs with many different effects using one compact data type.

Answering this question requires abstracting the concrete form of effect functors into a general program data type.
As we saw in the examples above, we need a way to represent pure values in a program.
Therefore, the first constructor of our new program data type should be \hinl{Return a} for the type \hinl{a}, that is, the result type of the program.
To model effects like non-determinism, the program is parameterized over effect functors of type \hinl{* -> *} that represent, for example, \hinl{Fail} and \hinl{Choice}.
We call this argument \hinl{sig} because the signature of a program tells us which effects can occur.
So far, programs are defined as \hinl{data Prog sig a = Return a}.
In order do make use of the \hinl{sig} component, we need to add a constructor for impure operations.
The \hinl{ND} data type shows us that effect functors can be defined recursively.
Thus, the constructor for impure programs should be recursive, too, to be able to represent this structure.

\begin{minted}{Haskell}
data Prog sig a = Return a | Op (sig (Prog sig a))
\end{minted}

With this definition of \hinl{Prog}, we are able to represent the original functors by instantiating \hinl{sig} appropriately.
For \hinl{Void}, we already have the \hinl{Return} constructor.
Therefore, the data type we can use with \hinl{Prog} does not need a constructor anymore, that is, \hinl{data Void' a}.

\begin{minted}{Haskell}
VoidProg a = Return a
           | Op (Void' (VoidProg a)) -- Void' has no constructors!
\end{minted}

The type \hinl{Prog Void'} now resembles the original type  \hinl{Void} since the \hinl{Op} constructor  would require a value of type \hinl{Void'}, which we cannot  construct.
\footnote{It is possible to use \hinl{undefined} to create an impure value of type \hinl{Prog Void' a}.
Since this  is not possible in Coq, we do not consider this in the Haskell implementation.}
Only \hinl{Return} can be used to define values, similar to the  original data type.

Similar to \hinl{Void'}, we can define a data type  \hinl{Choice} that represents \hinl{Choice} in combination with \hinl{Prog}.

\label{min:ND}
\begin{minted}{Haskell}
data ND p = Fail | Choice p p
\end{minted}

Again, we can omit the \hinl{Return} constructor because it is  already part of the \hinl{Prog} data type.
For the same reason,  the type variable \hinl{a} has been replaced with the variable \hinl{p}, since \hinl{ND} does not have values as arguments but rather programs that return values.

\begin{minted}{Haskell}
data NDProg a = Return a
              | Op (Choice (NDProg a))
\end{minted}

Since \hinl{Op} applies \hinl{sig} recursively,  this yields the following type, which is equivalent to the original data type.

\begin{minted}{Haskell}
data NDProg a = Return a
              | OpFail
              | OpChoice (NDProg a) (NDProg a)
\end{minted}

We have found a way to model effect functors as instances of the data type \hinl{Prog}, which essentially models a tree with leafs, represented by the \hinl{Return} constructor, and branches that have the form defined by \hinl{sig}.
\todo{Tree structure visualization?}

\section{Free Monads}
\begin{itemize}
\item What are free monads?
\item Why do we use free monads?
\end{itemize}

The data type \hinl{Prog} is better known as the \textit{free monad}.
We saw in the previous chapter that \hinl{Free} can be used to model other data types.
In addition, \hinl{Free} is a monad that can turn any functor into a monad.

We consider, for example, the type \hinl{Free One} where \hinl{data One a = One}.
Here \hinl{a} is a phantom type that we need because \hinl{Free} expects a functor.
The monad instance for \hinl{Free} is as follows.

\begin{minted}{Haskell}
data Free f a = Pure a | Impure (f (Free f a))

instance (Functor f) => Monad (Free f) where
  return = Pure
  Pure x >>= g = g x
  Impure fx >>= g = Impure (fmap (>>= g) fx)
\end{minted}

Since \hinl{One} has only a single, non-recursive constructor \hinl{One}, the only possible impure value is \hinl{Impure One}, whereas the usual \hinl{Return} constructor remains.
If bind encounters the value \hinl{One}, the function \hinl{g} is distributed deeper into the term structure using \hinl{fmap}.
Since \hinl{fmap One = One}, it becomes apparent that the monad constructed by \hinl{Free One} is the \hinl{Maybe} monad.
\todo{Visualization of fmap tree}

Since we want to model different effects in our program, the free monad makes writing programs easier by allowing monadic definitions without defining a separate monad instance for each effect.
\todo{Find more reasons for using free monads}
\todo{Stacking monads does not necessarily yield monads again}
% https://homepages.inf.ed.ac.uk/slindley/papers/handlers.pdf

\section{Modelling Effects}
\begin{itemize}
\item Explanation of the Prog/sig infrastructure
\item ND and state effect implementation 
\end{itemize}
\todo{Split ND into choice and fail instead of one}
In the previous sections the free monad and its ability to represent effect functors was discussed.
The goal of this section is to explore the infrastructure that allows us to combine multiple effects, write effectful programs and compute the result of such programs.

\subsection{Combining Effects}
Firstly, we would like to combine multiple effects.
For this purpose, we use the technique introduced by \citet{swierstra2008} to define a data type that combines the effect functors \hinl{sig1} and \hinl{sig2}.
The infix notation simplifies combining multiple effects via nested applications of \hinl{:+:}.

\begin{minted}{Haskell}
data (sig1 :+: sig2) a = Inl (sig1 a) | Inr (sig2 a)
\end{minted}

For example, the type \hinl{ND :+: One} is a functor that we can use with \hinl{Prog} to define programs that contain non-determinism and partiality as follows.

\begin{minted}{Haskell}
progNDOne :: Prog (ND :+: One) Int
progNDOne = Op (Inl (Choice (Op (Inr One)) (Return 42)))
\end{minted}

In the example \hinl{progNDOne} we define a program that represents the non-deterministic choice between a program whose value is absent and a program that returns \hinl{42}.
The complexity of nesting constructors of \hinl{Prog} and \hinl{:+:} correctly increases quickly for bigger terms.
Therefore, we define a type class that allows us to define such expressions more conveniently.
The class is parameterized over two functors, one of which is a subtype -- regarding \hinl{:+:} -- of the other.

\begin{minted}{Haskell}
class (Functor sub, Functor sup) => sub :<: sup where
  inj :: sub a -> sup a
\end{minted}

We need a few instances of the class \hinl{:<:} to make it useful.
The simplest case is \hinl{sig :<: sig} where want to inject a value of type \hinl{sig a} into the same type.
Since we do not need to modify the value in any way, \hinl{id} is used to define \hinl{inj}.

\begin{minted}{Haskell}
instance Functor sig => sig :<: sig where
  inj = id  
\end{minted}

The next instance covers the case \hinl{sig1 :<: (sig1 :+: sig2)}.
Since we already know that \hinl{sig1} is part of the sum type, we only need to apply the correct constructor of \hinl{:+:}, that is, \hinl{Inl} because \hinl{sig1} is the left argument.

\begin{minted}{Haskell}
instance (Functor sig1, Functor sig2) => sig1 :<: (sig1 :+: sig2) where
  inj = Inl
\end{minted}

The last instance assumes that we can inject \hinl{sig} into \hinl{sig2} and describes how we can inject \hinl{sig} into \hinl{sig1 :+: sig2}.
In this case, we can use \hinl{inj} to receive a value of type \hinl{sig2 a}.
All that remains is a situation similar to the previous instance, where we only need to use the matching constructor to complete the injection.
 
\begin{minted}{Haskell}
instance (Functor sig1, sig :<: sig2) => sig :<: (sig1 :+: sig2) where
  inj = Inr . inj
\end{minted}

These instances allow us to write a polymorphic definition of the function \hinl{inject} which injects constructors depending on the given type of the program.

\begin{minted}{Haskell}
inject :: sig1 :<: sig2 => sig1 (Prog sig2 a) -> Prog sig2 a
inject = Op . inj
\end{minted}

\hinl{inject} can then be used as demonstrated in the following example.

\begin{minted}{Haskell}
λ> inject One :: Prog (One :+: ND) a
Op (Inl One)
λ> inject One :: Prog (ND :+: One) a
Op (Inr One)
\end{minted}

The implementation of the function \hinl{inject} assumes that we can inject \hinl{sig1} into \hinl{sig2}.
This is because \hinl{sig2} is the signature of the returned program and \hinl{sig1} is the type of the effect constructor that we want to inject.
This restriction is justified because, for example, non-deterministic syntax should only appear in a program where \hinl{ND} is part of the signature.
With this part of the infrastructure in place, we can redefine the example \hinl{progNDOne} without using \hinl{Inl} and \hinl{Inr} explicitly.

\begin{minted}{Haskell}
progNDOne' :: Prog (ND :+: One) Int
progNDOne' = inject (Choice (inject One) (Return 42))
\end{minted}

Deriving the appropriate instance of \hinl{:<:} when using \hinl{inject} is, however, not always unambiguous.
The last two instances overlap in situations where \hinl{sig = sig1}.
For example, the example \hinl{inject One :: Prog (One :+: One) a} yields different values with respect to the chosen constructor of \hinl{:<:}, depending on the instance.

\begin{minted}{Haskell}
λ> nothing
Op (Inl One) -- second instance
λ> nothing
Op (Inr One) -- third instance
\end{minted}

This is because the type constraint of \hinl{inject}, in this case \hinl{One :<: (One :+: One)}, matches both the second and third instance.
Haskell does not accept overlapping instances by default, which is why we prioritize one instance via pragmas.
In practice, the different term structure due to \hinl{Inl} and \hinl{Inr} does not influence the evaluation as long as we do not explicitly match for the constructors.
This is ensured by an additional function \hinl{prj} of the type class \hinl{:<:}, which is discussed in the next section.

\subsection{Simplified Pattern Matching}
While the function \hinl{inject} allows us to write programs in a more convenient way, we also need to consider how we can evaluate programs.
The same issue of nested applications of \hinl{Op}, \hinl{Inl} and \hinl{Inr} applies when we want to distinguish different effects via pattern matching.
Thus, we add a second function \hinl{prj} to the type class \hinl{:<:}.

\begin{minted}{Haskell}
class (Functor sub, Functor sup) => sub <: sup where
  inj :: sub a -> sup a
  prj :: sup a -> Maybe (sub a)
\end{minted}

The function \hinl{prj} is a partial inverse to \hinl{inj}.
This means that we can project values of a type \hinl{sup a} into a subtype \hinl{sub a}.
For this reason, the return type of the function is a \hinl{Maybe} type.
Similar to \hinl{inj}, we have to define instances for the same cases as before.

\begin{itemize}
\item For \hinl{sig :<: sig}, we can define \hinl{prj} as \hinl{Just} because we know that every element of the supertype is also an element the subtype.

\item \hinl{sig1 :<: (sig1 :+: sig2)} means that we can return \hinl{Just x} for \hinl{Inl x}.
However, for \hinl{Inr} we need to return \hinl{Nothing} because we cannot, in general, project from \hinl{sig2} to \hinl{sig1}.

\item In the last case \hinl{sig :<: sig2 => sig :<: (sig1 :+: sig2)} we know that we can project from \hinl{sig2} to \hinl{sig}.
Thus, in case of \hinl{Inr x}, where \hinl{x} has the type \hinl{sig2}, we can apply \hinl{prj} to construct a value of appropriate type.
The other case \hinl{prj (Inl _)} is handled by returning \hinl{Nothing}.
\end{itemize}

With the definition of \hinl{prj} and the instances of \hinl{:<:}, we can now define the function \hinl{project} which we can use to make pattern matching more convenient.

\begin{minted}{Haskell}
project :: (sub :<: sup) => Prog sup a -> Maybe (sub (Prog sup a))
project (Op s) = prj s
project _      = Nothing
\end{minted}

Due to the recursive definition of the \hinl{Prog} data type, constructors like \hinl{Choice} have \hinl{Prog} arguments themselves.
Thus, \hinl{sub} is applied to \hinl{Prog sup a} in the return type of the projection.
We can only project effectful values because generally it is not clear which functor we should choose for \hinl{sub} when projecting a \hinl{Return} value.

Finally, we can now inject and project effectful values.
Since \hinl{project} is a partial inverse of \hinl{inject}, the equation \hinl{project (inject x) = Just x} holds for values \hinl{x} of appropriate type, excluding failing computations.
\todo{Does it hold?}
This is demonstrated in the following example.

\begin{minted}{Haskell}
λ> type T = Maybe (ND (Prog (ND :+: One) Int))
λ> project (inject (Choice (Return 42) (Return 43))) :: T
Just (Choice (Return 42) (Return 43))
\end{minted}

Now that we can use \hinl{project} as an abstraction of the concrete term structure regarding \hinl{:<:}, we can write a first function that evaluates a non-deterministic, partial program.

\begin{minted}{Haskell}
evalNDOne :: Prog (ND :+: One) a -> [a]
evalNDOne (Return x) = [x]
evalNDOne p = case project p of
               Just (Choice p1 p2) -> evalNDOne p1 ++ evalNDOne p2
               Just Fail           -> []
               Nothing             -> case project p of
                                        Just One -> []
                                        Nothing  -> []
\end{minted}

When \hinl{evalNDOne} encounters a value \hinl{Return x}, \hinl{x} is returned as a singleton list.
For effectful programs, we can use \hinl{project} to distinguish between the constructors of \textit{one effect at a time}.
The case patterns hold the necessary type information for \hinl{project}.
When the projection returns \hinl{Nothing}, another effect can be matched in a nested case expression.
Since we never need to explicitly match for \hinl{Inl} or \hinl{Inr}, overlapping patterns in the instances of \hinl{:<:} do not affect the evaluation of programs in our model.

Although we have already eliminated \hinl{Inl}, \hinl{Inr} and \hinl{Op} from functions that create or evaluate programs, there can be done even more to simplify programming with effects.
Two language extensions, \hinl{PatternSynonyms} and \hinl{ViewPatterns}, allow us to write definitions like the following.

\begin{minted}{Haskell}
pattern PChoice p q <- (project -> Just (Choice p q))
\end{minted}

View patterns -- the right-hand side of the \hinl{<-}, make pattern-matching for certain cases more compact.
A view pattern consists of a function on the left-hand side of \hinl{->}, that is applied to the value that the pattern is matched against, and a pattern on the right-hand side.
The result of the function call is matched against this pattern and the variables inside the pattern can be used in the definition.
The function \hinl{evalNDOne} can be defined using view patterns in the following way.

\begin{minted}{Haskell}
evalNDOne' :: Prog (ND :+: One) a -> [a]
evalNDOne' (Return x) = [x]
evalNDOne' (project -> Just (Choice p1 p2)) = evalNDOne' p1 ++ evalNDOne' p2
evalNDOne' (project -> Just Fail          ) = []
evalNDOne' (project -> Just One           ) = []
\end{minted}

We cannot use \hinl{(project -> Nothing)} without type annotations as a pattern because this would result in overlapping instances.
However, no effects other than those specified in the signature can occur within the program.
Therefore, the \hinl{Nothing} pattern is not necessary.

The second component of the pattern definition above is the option to define a synonym for more complex patterns.
In this case, we name the view patterns similar to the original constructors of the effects.
While this is necessary for every effect constructor, it allows us to rewrite the definition in the following way.

\begin{minted}{Haskell}
evalNDOne'' :: Prog (ND :+: One) a -> [a]
evalNDOne'' (Return    x) = [x]
evalNDOne'' (PChoice p q) = evalNDOne'' p ++ evalNDOne'' q
evalNDOne'' (PFail      ) = []
evalNDOne'' (POne       ) = []
\end{minted}

Writing programs that evaluate effectful programs is now almost as convenient as simple pattern matching.
Finally, a useful definition for working with programs that have the signature \hinl{f :+: g}, where want to match for \hinl{f} but not \hinl{g}, is as follows.

\begin{minted}{Haskell}
pattern Other s = Op (Inr s)
\end{minted}

Since \hinl{:+:} is right-associative in nested applications, we can match for the left argument effect and conveniently match all remaining effects with \hinl{Other}.


\subsection{Effect Handlers}
\label{sec:effectHandlers}
For each effect in a program's signature, a \textit{handler} is required.
Handling an effect means transforming a program that contains a certain effect into a program where the effect's syntax does not occur anymore.
However, the syntax is not just removed, but the effect's semantics is applied.
The semantics of an effect is therefore given by its handler.
In the following we discuss handlers for the effects non-determinism and state.

\paragraph{Void Effect}
We begin with the data type for absence of effects, \hinl{Void}.
Due to its definition without constructors, there is no \hinl{Void} syntax that needs to be handled.
The only constructor for programs with the signature \hinl{Void} is \hinl{Return}, which we can handle by returning the argument.
Thus, the handler for \hinl{Void} removes the program layer and is usually applied last, when all other effects have been handled.

\begin{minted}{Haskell}
run :: Prog Void a -> a
run (Return x) = x
\end{minted}

\paragraph{Non-determinism Effect}
We already defined a data type for non-deterministic programs in \autoref{min:ND}.
The \hinl{Choice} constructor did not contain any IDs, which we need for the implementation of call-time choice.
Thus, the revised data type is as follows.

\begin{minted}{Haskell}
data ND p = Fail | Choice (Maybe ID) p p
\end{minted}

Not every non-deterministic choice in a program needs an ID, since IDs slow down the evaluation of choices considerably.
Thus, IDs are optional and only assigned when necessary, that is, when choices are shared.

In the last section, we already defined a function \hinl{evalNDOne} that handles the simple \hinl{ND} type without IDs by returning a list of results, where, for each choice, the result lists are concatenated.
For choices with IDs, however, this is not sufficient.
We begin by transforming the program into a program that returns a tree data type which mirrors the non-determinism structure.
\todo{Keep tree structure?}
 
\begin{minted}{Haskell}
runND :: (Functor sig) => Prog (ND :+: sig) a -> Prog sig (Tree.Tree a)
runND (Return a) = return (Tree.Leaf a)
runND Fail       = return Tree.Failed
runND (Choice m p q ) = do
  pt <- runND p
  qt <- runND q
  return (Tree.Choice m pt qt)
runND (Other op) = Op (fmap runND op)
\end{minted}

Next, we need to memorize the decisions that were made while traversing the choice tree.
For this reason, we define a data type \hinl{Decision} that indicates whether the left or right branch of a choice has been picked before for a particular choice ID.
A \hinl{Memo} is maps IDs to decisions.

\begin{minted}{Haskell}
data Decision = L | R
type Memo = Map.Map ID Decision
\end{minted}

The depth-first traversal of the choice tree is implemented in the function \hinl{dfs}.
The returned list of results is created similar to the approach in \hinl{evalNDOne}, except for the case where a choice has a non-empty ID.
The ID could have appeared in a choice that is closer to the root node of the tree and thus, the choice could have already been decided.
Therefore, we need to look up the ID in the \hinl{Memo}.
If the choice has not been made yet, that is, \hinl{Nothing} is returned, the \hinl{Memo} is updated with \hinl{L} for the left branch and \hinl{R} for the right branch.
The recursive calls then descend into the corresponding branch and will make the same decision for this ID if it occurs again.
If, on the other hand, a decision is returned by the \hinl{lookup} function, the branch of the recursive call is chosen according to the decision.

\begin{minted}{Haskell}
dfs :: Memo -> Tree a -> [a]
dfs mem Failed = []
dfs mem (Leaf x) = [x]
dfs mem (Choice Nothing t1 t2) = dfs mem t1 ++ dfs mem t2
dfs mem (Choice (Just n) t1 t2) =
    case Map.lookup n mem of
      Nothing -> dfs (Map.insert n L mem) t1 
              ++ dfs (Map.insert n R mem) t2
      Just L -> dfs mem t1
      Just R -> dfs mem t2
\end{minted}

The function \hinl{dfs} is called with an empty map and yields the list of results that the choice tree represents.
\todo{Examples}

\paragraph{State Effect}
Stateful computations are an important part of the sharing effect that is presented in \autoref{sec:sharing}.
We begin by defining the syntax of the state effect.
Usually, stateful computations can read the current state with \hinl{get} and set a new state with \hinl{put}.
Thus, the data type needs those two constructors, too.
We add an additional type variable that abstracts the type of values that the state can hold.
The variable \hinl{p} represents the program type as before.

\begin{minted}{Haskell}
data State s p = Get' -- ?
               | Put' -- ?
\end{minted}

Both constructors have an effect on a program, that is, a scope in which the effects are visible.
Thus, both constructors need a program argument \hinl{p}.
For \hinl{Put'}, we can simply add the arguments \hinl{s} for the new state and \hinl{p} for the program in which the new state is set.
The constructor \hinl{Get'}, however, contains the program in a different form, namely a function \hinl{s -> p}.
The reason for this is that, if we were using a simple \hinl{p} argument, the handler would have to somehow replace all \hinl{get}-occurrences of the state with appropriate values.
This would require evaluating the whole program, which would defeat the purpose of preserving non-strictness.
Hence, the program is added to \hinl{get} in the form of a functional expression where the function argument replaces the occurrences of the state that are being read in the program.
The data type for the state effect now looks as follows.

\begin{minted}{Haskell}
data State s p = Get' (s -> p)
               | Put' s p
\end{minted}

The smart constructors for stateful programs are defined by instantiating the program and function arguments appropriately.
For \hinl{get}, this means that we need to supply a function of type \hinl{s -> Prog sig s} since \hinl{p} is \hinl{Prog sig s} in this context.
Conveniently, the \hinl{return} function matches this type and thus, is the initial argument of \hinl{Get'}.
For \hinl{put}, the new state and a program that returns \hinl{()} are supplied to \hinl{Put'} because \hinl{put} does not return any information.

\begin{minted}{Haskell}
get :: (State s :<: sig) => Prog sig s
get = inject (Get' return)

put :: (State s :<: sig) => s -> Prog sig ()
put s = inject (Put' s (return ())
\end{minted}

The choice of initial function arguments might not seem intuitive at first because it is not clear how the remaining program finds its way into the argument of, for example, \hinl{Get'}.
Therefore, we consider an example of the state effect and how the free monad is used to write programs.

\begin{minted}{Haskell}
p :: Prog (State Int :+: Void) Int
p = do put 42
       i <- get
       return (i * 2)
\end{minted}

The program sets a state \hinl{42}, gets the value of the current state and then returns double of that.
The normal form of \hinl{p} can be computed by evaluating the occurrences of bind.
We recall the monad instance for the free monad: bind uses \hinl{fmap} to distribute a function deeper into a term.
Thus, we first define a \hinl{Functor} instance.

\begin{minted}{Haskell}
instance (Functor sig) => Monad (Prog sig) where
  return x = Return x
  Return x >>= f = f x
  Op op >>= f = Op (fmap (>>= f) op)

instance Functor (State s) where
  fmap f (Get' g)   = Get' (f . g)
  fmap f (Put' s p) = Put' s (f p)
\end{minted}

In the case of \hinl{Get'}, we need to apply \hinl{g} to a state in order to obtain a program that we can apply \hinl{f} to.
Thus, we pass the result from \hinl{f} to \hinl{g} via function composition.
For \hinl{Put'}, the state \hinl{s} remains unmodified and the function \hinl{f} is applied to the program argument \hinl{p} of the constructor.

Now we can transform the program \hinl{p} into normal form as follows.

\begin{minted}[fontsize=\footnotesize]{Haskell}
put 42 >>= \_ -> get >>= \i -> return (i * 2)
= inject $ fmap (>>= \_ -> get >>= \i -> return (i * 2)) (Put' 42 (return ()))
= inject $ Put' 42 (get >>= \i -> return (i * 2))
= inject $ Put' 42 (inject $ fmap (>>= \i -> return (i * 2)) (Get' return))
= inject $ Put' 42 (inject $ (Get' (\i -> return (i * 2))))
\end{minted}

\hinl{Op} as well as \hinl{Inl} and \hinl{Inr} constructors are replaced by \hinl{inject} in this example.
The expression is transformed by applying the definitions of bind and \hinl{fmap}.
In the last step, we simplify the expression by applying the left identity monad law, that is, \hinl{(>>= f) . return = f}.

We can now see that the remaining program after \hinl{get}, that is, the \hinl{return} call, has been moved into the argument function of \hinl{Get'}.
The function expects a state and replaces the variables, that were bound to the return value of \hinl{get} in the original program, with the state.

Now that we have seen the definition of stateful program syntax and how the state flows through the program via functions, we can define the handler for the state effect.
Naturally, the handler needs to keep track of the current state, which is the first argument of the function.
Then, the function expects a program that contains state syntax.
Finally, the return type is a program that returns a pair of the current state and a return value.

\begin{minted}{Haskell}
runState :: Functor sig => s -> Prog (State s :+: sig) a -> Prog sig (s, a)
runState s (Return a) = return (s, a)
runState s (Get    k) = runState s (k s)
runState s (Put s' k) = runState s' k
runState s (Other op) = Op (fmap (runState s) op)
\end{minted}

For pure values, the current state and the value inside the \hinl{Return} constructor is returned.
When a \hinl{Get} is encountered, we apply the function argument, which expects a state, to the current state and do a recursive call with the resulting program.
\hinl{Put} is handled by a recursive call where the old state is replaced by the new state while the program stays the same.
Finally, other syntax is handled by using \hinl{fmap} to distribute the handler deeper into the term structure, similar to the other handlers we have seen.

The example program \hinl{p} can now be handled by first calling the handler \hinl{runState} to handle the state effect, followed by \hinl{run} to extract the result from the program structure.

\begin{minted}{Haskell}
λ> run . runState 1 $ p
(42,84)
\end{minted}

As expected, the first component represents the current state, which was set by \hinl{put} to \hinl{42}, while the second component is the result that was returned after multiplying the current state by two.

\subsection{Handling Order}
When multiple effects are part of the signature, the question arises whether running handlers in a different order has an effect on the result.
As an example, we define a handler that does not remove syntax but actually adds state syntax to a non-deterministic program.
The function \hinl{results} keeps the structure of a program intact but adds state syntax that increments the current state by one for each result.

\begin{minted}{Haskell}
results :: (ND <: sig, State Int <: sig) => Prog sig a -> Prog sig a
results (Return x)      = get >>= put . (+ 1) >> return x
results Fail            = fail
results (Choice m p q ) = choiceID m (results p) (results q)
results (Op op)         = Op (fmap results op)
\end{minted}

Now we define a program \hinl{tree} that builds the complete, binary choice tree of height \hinl{x}.
For each call of \hinl{tree}, a choice is made where the current height is either incremented or decremented by one.

\begin{minted}{Haskell}
tree :: (ND <: sig) => Int -> Prog sig Int
tree 0 = return 0
tree x = tree (x - 1) >>= \i -> 
  choice (return $ i + 1) (return $ i - 1)
\end{minted}

Each time choice is called, two new branches are created.
Thus, we expect \hinl{tree x} to have $2^\text{\hinl{x}}$ results.
To see the program in action, we define two handlers.
The difference between \hinl{treeGlobal} and \hinl{treeLocal} is the order of the handlers.
In both cases \hinl{results} is run first, but whereas \hinl{treeGlobal} runs the non-determinism handler before the state handler, the opposite is true for \hinl{treeLocal}.

\begin{minted}{Haskell}

treeGlobal :: (Int, Tree.Tree Int)
treeGlobal = run . runState 0 . runND . results $ tree 2

treeLocal :: Tree.Tree (Int, Int)
treeLocal = run . runND . runState 0 . results $ tree 2
\end{minted}

The types of the definitions already indicate a difference.
While \hinl{treeGlobal} returns a state paired with a tree of results, \hinl{treeLocal} returns a tree of state and result pairs.
In the following, the result of evaluating each handler chain is presented as a visualization of the resulting choice tree.

\vspace{0.32cm}

\begin{minipage}{.465 \linewidth}
\begin{minted}{Haskell}
λ> putStrLn . pretty $ treeGlobal
(4,?
├─── ?
│    ├─── 2
│    └─── 0
└─── ?
     ├─── 0
     └─── -2)
\end{minted}
\end{minipage}
\hfill
\vline
\hfill
\begin{minipage}{.475 \linewidth}
\begin{minted}{Haskell}
λ> putStrLn . pretty $ treeLocal
?
├─── ?
│    ├─── (1,2)
│    └─── (1,0)
└─── ?
     ├─── (1,0)
     └─── (1,-2)
\end{minted}
\end{minipage}

\vspace{0.32cm}

As the name suggests, \hinl{treeGlobal}, that is, handling non-determinism first and state second, evaluates the program with a global state, where each non-deterministic branch shares the same state.
Contrary to that, \hinl{treeLocal} creates an individual state for every non-deterministic branch by handling state syntax first.
While the results are not influenced by the order of handlers in this case, this is not generally the case.

\section{Implementing Scoped Effects}
\label{sec:sharing}
\begin{itemize}
\item How can we implement simple sharing as an effect?
\item What about deep/nested sharing?
\item Examples (exRecList, ...)
\end{itemize}

Although Haskell offers sharing as part of the language, we have seen in \autoref{sec:sharingComputations} that the built-in sharing mechanism does not always work as intended when combined with lifted data types.
Thus, we need to model sharing as an effect using the tools that were presented in the previous section.
There is, however, a difference between sharing and the other effects we have seen so far.
Sharing is not an independent effect since it affects non-deterministic choices.
This means that, depending on the presence of sharing, some choice branches may not be explored.
Therefore, sharing is a \textit{scoped} effect, that is, only a delimited part of the program is affected by the effect.

\citet{wu2014effect} present two ways to define scoped effects.
Firstly, syntax for explicitly marking the begin and end of a scope can be defined.
This leads to a more complicated handler because the begin and end tags can be mismatched in the program and one needs to keep track of the current scope environment.
The second approach uses higher-order syntax, that is, the signature of a program is not just a functor but a function that takes a functor as an argument.
This approach makes it possible to have the scoped program as an argument of the syntax constructor.
In the following, an overview of a  -- initially promising but ultimately incorrect  -- hybrid approach and both options mentioned before is given.

\subsection{Hybrid Implementation}

The idea of the hybrid implementation is a combination of the explicit scoping infrastructure and direct program arguments in the syntax definition that the higher-order implementation uses.
In theory, this has the benefit of simple handlers and scoping via program arguments instead of explicit tags.
Therefore, it seemed worthwhile to explore this approach instead of following one of the options mentioned in the introduction of the section.

Beginning with the definition of the sharing syntax data type, we follow the idea of the higher-order approach and define a single constructor \hinl{Share'} with a program argument that represents the shared program.
Although \hinl{p} is supposed to be only the shared program, the monadic bind structure moves the program that follows the \hinl{Share'} constructor into the argument \hinl{p}
The same happened in \autoref{sec:effectHandlers} for the program argument of the state effect constructor \hinl{put}.

\begin{minted}{Haskell}
data Share p = Share' p

share :: (Share :<: sig) => Prog sig a -> Prog sig (Prog sig a)
share p = return $ inject (Share' p)
\end{minted}

The return type of \hinl{share} is not just a program but a program that returns a program.
The reason for this is explained later in \autoref{subsec:sharingImplementation}.
For the first implementation of \hinl{share}, this outer program layer is empty and thus created by \hinl{return}.

In order to create an example that showcases the usage of \hinl{share} and the monadic structure, we need a few definitions.
First, we define a non-deterministic coin that returns either \hinl{0} or \hinl{1} and a lifted addition function for programs that return integer results.
Since \hinl{(+)} is a strict function in Haskell and Curry, we can mirror this behavior by binding both program arguments and then adding the results.

\begin{minted}{Haskell}
coin :: (ND :<: sig) => Prog sig Int
coin = choice (return 0) (return 1)

addM :: (Functor sig) => Prog sig Int -> Prog sig Int -> Prog sig Int
addM p q = p >>= \i -> q >>= \j -> return (i + j)
\end{minted}

With these functions defined, we can now use the \hinl{share} operator to add a shared coin to an unshared coin, twice, as shown in the following example.
This corresponds to the Curry code \hinl{let x = coin in (x + coin) + (x + coin)}.

\begin{minted}{Haskell}
exAddSharedCoinTwice :: Prog (Share :+: ND :+: Void) Int
exAddSharedCoinTwice = share coin >>= \fx -> addM (addM fx coin) 
                                                  (addM fx coin)
\end{minted}

\vspace{0.32cm}

\begin{minipage}{.4 \linewidth}
\begin{minted}[escapeinside=||, fontsize=\footnotesize]{Haskell}
< ?|$_\texttt{1}$|
  ├── ?|$_\texttt{2}$|
  │   ├── < ?|$_\texttt{1}$|
  │   │     ├── ?|$_\texttt{2}$|
  │   │     │   ├── 0 > >
  │   │     │   └── 1 > >
  .   .     └── ?|$_\texttt{2}$|
  .   .         ├── 1 > >
  .   .         └── 2 > >
\end{minted}
\begin{center}
Hybrid implementation
\end{center}
\end{minipage}
\hspace{.1 \linewidth}
\vline
\hspace{.1 \linewidth}
\begin{minipage}{.475 \linewidth}
\begin{minted}[escapeinside=||, fontsize=\footnotesize]{Haskell}
< ?|$_\texttt{1}$|
  ├── > ? 
  │     ├── < ?|$_\texttt{1}$|
  │     │     ├── > ? 
  │     │     │     ├── 0
  │     │     │     └── 1
  .     .     └── > ? 
  .     .           ├── 1
  .     .           └── 2
\end{minted}
\begin{center}
Explicit scoping tags
\end{center}
\end{minipage}

\vspace{0.32cm}

The left-hand side tree is generated using the data type \hinl{Share} with a single constructor, while the right-hand side visualizes a data type with two constructors that explicitly delimit the scope.
Subscript numbers represent the ID of a choice.
Although this information is added by a sharing handler we have not defined yet, choice IDs are important in order to understand the consequences of the data type definitions for the sharing effect.

Choice IDs are assigned inside a sharing scope.
When a sharing scope is duplicated due to the monadic structure, the choices inside get the same IDs.
Finally, when the choice tree is evaluated, these choices are linked.
The right-hand side tree shows us that explicit scoping tags allow ending a scope in a program.
For example, the scope around the root choice ends first and then the next scope is opened.
The visualization of the hybrid term shows that all opened sharing scopes are only closed at the end of each branch.
This difference in term structure means that the handler for the hybrid approach never stops assigning IDs to choices because it cannot distinguish the shared program that was initially passed as an argument and the following program that was moved into the argument by the monadic structure.

The hybrid implementation correctly assign the ID \hinl{1} for the choice that immediately follows the beginning of the scope.
This is the shared choice that is defined in \hinl{coin}.
The next choice within the branch originates from the unshared coin and ideally should not receive an ID.
Indeed, the implementation with explicit \hinl{begin} and \hinl{end} tags closes the sharing scope after the first choice and thus, the choice does not receive an ID.
The hybrid implementation, however, cannot stop assigning IDs to choices and thus assigns \hinl{2} to the choice.

In the hybrid implementation, when a new scope is opened, the current scope is overwritten.
For this example, this means that the next choice is labeled with \hinl{1} again, since each scope is associated with an initial state that is copied, too, when the sharing scope is duplicated.
Because there is only one sharing scope in the original program, all occurring scopes are duplicates that were created due to non-determinism.
It is critical that copied sharing scopes behave identical because this ensures that the choices inside the scopes are named the same way, resulting in correct call-time choice behavior.
In the example, however, this leads to a fatal flaw.
Until now, assigning the ID \hinl{2} to the unshared choice below the root choice was unnecessary but not incorrect.
As a consequence of the second sharing scope behaving identical to the first one, the second unshared choice also receives the ID \hinl{2}.
Since we now have two equal IDs within a branch, this means that the second choice with the ID \hinl{2} is linked to the decision of the first choice with the ID \hinl{ID}, that is, the first unshared \hinl{coin} is linked to the second one.

This was not intended in the original program and proves that the hybrid approach is unsuitable for modelling scoped effects and, consequently, sharing.
Interestingly, this approach promisingly passed all example tests and algorithms in both Haskell and Coq.
The flaw was only found while doing the finishing touches on the ID generation algorithm.
This shows that the hybrid approach is not incorrect in its entirety but merely requires some refinement, as shown in the next subsection.

\subsection{Higher-Order Scope Syntax}
The higher-order approach described by \citet{wu2014effect} is based on a modified  program data type to represent scoped syntax.
So far, the type variable \hinl{sig} has been a functor that is applied to the program type again.
In the higher-order data type, however, \hinl{sig} is applied to a program functor and a type, which makes it of type \hinl{(* -> *) -> * -> *}.

\begin{minted}{Haskell}
data Prog   sig a = Return a | Op (sig (Prog sig a))
data ProgHO sig a = Return a | Op (sig (Prog sig) a)
\end{minted}

Due to the functor argument of \hinl{sig}, it is now called a higher-order functor.
Based on the new program type and higher-order functors, the existing infrastructure for combining signatures, injecting values and pattern matching can be adapted.
This is not discussed here since we are mostly interested in the the definition of effect data types.
For example, the higher-order version of the sharing effect is defined as follows.

\begin{minted}{Haskell}
data HShare m a = forall x. Share' (m x) (x -> m a)
\end{minted}

Due to the new type of \hinl{sig} in the definition of programs, effect data types have an additional argument now, too.
The single argument \hinl{p} has been replaced by a functor argument \hinl{m} and a type \hinl{a}.
Applying \hinl{m} to \hinl{a} corresponds to the argument \hinl{p} we have seen in the previous effect types.
One advantage of splitting \hinl{p} is that it is now possible to apply \hinl{m} to different types, whereas we were limited to \hinl{p} before.
\citet{wu2014effect} demonstrate that this can be useful, for example, when defining exceptions with \hinl{throw} and \hinl{catch} syntax.
Syntax for \hinl{catch} usually consists of a program where exceptions may occur, a handler for said exceptions and the remaining program.
This structure is very similar to the sharing effect since we would also like to pass the shared program as an argument to the sharing syntax.
However, this was not possible with functor-based program type, as we have seen in the previous subsection.
With higher-order programs, however, we can represent the shared program as an argument of type \hinl{m x} where \hinl{m} represents \hinl{Prog sig} and \hinl{x} the return value of the program.
The remaining program is a continuation function \hinl{x -> m a}that takes the result of the shared program and substitutes the results of all matching calls of \hinl{share}, similar to how the current state is propagated in the program for the state effect.

The purpose of {forall x} lies in adding an independent type variable using the language extension \hinl{ExistentialQuantification}.
In this case, independence means that the variable does not occur on the left-hand side of the definition and thus can be different for two values of the same type.
For example, the following data type has a regular type variable and one introduced by \hinl{forall}.

\begin{minted}{Haskell}
data Test a = forall x. Test x a

instance Functor Test where
  fmap f (Test x a) = Test x (f a)
\end{minted}

With this definition, \hinl{[Test 42 True, Test () False]} is a valid expression of type \hinl{Test Bool}.
When we define a functor instance for \hinl{Test}, the argument \hinl{x} remains unmodified while \hinl{f} is applied to \hinl{a}.
Although in a different form, this applies to the sharing data type as well.
The call of \hinl{fmap}, or rather the higher-order equivalent \hinl{emap}, in the definition of bind is responsible for building the program structure and thus, appends the remaining program to the shared program in the case of the definition we used for the hybrid implementation.
Since \hinl{emap} transforms a value of type \hinl{Share m a} into a \hinl{Share m b}, there is no way to leave one program argument (the shared program) unmodified while applying a function to the other.
For this reason, the additional, independent type variable \hinl{x} is necessary in the definition of the sharing effect data type.

One disadvantage of the higher-order approach is the more complicated infrastructure and effect handlers.
In short, \hinl{Other} cases are harder to handle because the simple \hinl{fmap}-approach does not work anymore.
Additionally, due to the function argument of \hinl{Share'}, the visualization of sharing scopes and programs becomes difficult.
Therefore, we will pursue the explicit scoping syntax approach for the remainder of the Haskell chapter.

\subsection{Explicit Scope Syntax}
\label{subsec:explicit}
The previous subsections have demonstrated that program arguments do not correctly model scopes unless we use higher-order infrastructure.
Thus, an alternative approach is needed.
A well known syntactical structure for delimiting scopes are explicit scope tags in the form of \hinl{begin} and \hinl{end} or brackets.
Following this idea, we split the sharing syntax into two parts.
One constructor marks the beginning of the scope, while the other marks the ending of the scope.

\begin{minted}{Haskell}
data Share p = BShare' p | EShare' p
\end{minted}

Both constructor have programs arguments.
\hinl{BShare'}s argument program contains the scoped program block followed by an \hinl{Eshare'} with the remaining program as an argument.
Similar to the state effect, our smart constructors use \hinl{return ()} as an initial program that is replaced by the actual program when the bind structure is evaluated.

\begin{minted}{Haskell}
begin :: (Share :<: sig) => Prog sig ()
begin = inject (BShare' (return ()))

end :: (Share :<: sig) => Prog sig ()
end = inject (EShare' (return ()))
\end{minted}

For example, the following expression shows a scope that includes the \hinl{Choice'} constructor but not the \hinl{Return} values.

\begin{minted}{Haskell}
inject $ BShare' (inject $ Choice' Nothing 
                                   (inject $ EShare' (Return 0))
                                   (inject $ EShare' (Return 1)))
\end{minted}

Now that we can delimit the scope of the sharing effect, it is time to define the actual sharing operator.


\begin{minted}{Haskell}
share :: (Share :<: sig) => Prog sig a -> Prog sig (Prog sig a)
share p = return $ do begin ; x <- p ; end ; return x
\end{minted}

\hinl{share} wraps \hinl{begin} and \hinl{end} tags around a call of bind that executes the program \hinl{p}.
Then, the result is returned.
One problem of this approach is that sharing tags can be mismatched.
For this reason, sharing syntax should only be accessible by means of the smart constructor \hinl{share}.
Nevertheless, mismatched scoping tags are part of the syntax definition and need to be handled.

Now that we have defined the syntax of the sharing effect with explicit scope constructors, we need to consider how the handler should work.
From the structure of the syntax follows that the handler needs to extract the scoped program between the \hinl{begin} and \hinl{end} tags and then modify the choices that occur inside the scope.
Following this idea, we divide the sharing handler into two parts.
The first part is \hinl{bshare}, a function that waits for a \hinl{begin} tag and then hands over its program argument to \hinl{eshare}, which handles the scope and finally returns the program that follows the scope.
Since this program is now outside of the scope, \hinl{bshare} waits for the next \hinl{begin} tag without modifying any choices.

\begin{minted}{Haskell}
bshare :: (ND <: sig) => Prog (Share + sig) a -> Prog sig a
bshare (Return a) = return a
bshare (BShare p) = eshare p >>= bshare
bshare (EShare p) = error "mismatched Eshare"
bshare (Other op) = Op (fmap bshare op)
\end{minted}

The case of mismatched scoping tags, that is, an \hinl{Eshare} occurring before a \hinl{BShare} has opened a scope, can be handled in Haskell with a run-time error.
In Coq, however, this is not possible.
We could wrap the return type of the function in \hinl{Maybe} to represent mismatched tags, but this makes proofs more cumbersome due to the added case distinction.
A solution to this problem is discussed in the next chapter about modelling call-time choice in Coq.

The second part of the handler handles the scoped program and thus should modify choices in such a way that the program behaves as expected regarding call-time choice.

\begin{minted}{Haskell}
eshare :: (ND <: sig)
       => Prog (Share + sig) a -> Prog sig (Prog (Share + sig) a)
eshare (Return a)     = return (Return a)
eshare (BShare p)     = eshare p
eshare (EShare p)     = return p
eshare Fail           = fail
eshare (Choice _ p q) = choiceID {- ID? -} (eshare p) (eshare q)
eshare (Other op)     = Op (fmap eshare op)
\end{minted}

Pure values are simply returned.
When a \hinl{begin} tag is found, this means that there is a scope within in scope, that is, nested scopes.
In this case, \hinl{eshare} keeps modifying choices because neither the original scope nor the new one has not been closed yet.
Contrary to that, closing tags result in switching back to \hinl{bshare} for the remaining program.
Finally, when a choice is encountered, an ID needs to be created for  \hinl{choiceID}, a function which creates a choice with an explicitly passed ID.
However, this is a problem.

The ID that the choice came with is always \hinl{Nothing} because choices are created without IDs.
It comes to mind that \hinl{eshare} could have a state that is incremented for each encountered choice.
Unfortunately, this would entail that each choice is assigned a different ID, that is, two choices could never have the same ID.
This defeats the purpose of choice IDs because it makes sharing impossible.

Consequently, the main finding from the first attempt to define the sharing handler is that we need to add an identifier to sharing scopes.
This allows linking scopes that were duplicated due to non-determinism in the program and can be used to create choice IDs.
Since the problem of linking scopes is more relevant to the implementation of the sharing effect than scoped effects in general, it is discussed in the next section.

\section{Implementation of Sharing as Effect}
\label{subsec:sharingImplementation}
In this section, the simple implementation of sharing from the previous section is refined into an implementation that models call-time choice correctly.

\subsection{Sharing IDs}
To begin with, we consider the following example that shows why we need to link sharing scopes.

\begin{minted}{Haskell}
exAddSharedCoin :: Prog (Share :+: ND) Int
exAddSharedCoin = share coin >>= \fx -> addM fx fx
\end{minted}

The \hinl{coin} in the addition is shared and thus, the expected result is \hinl{0} and \hinl{2}.
When represented as a tree, the example looks like the following.

\begin{minted}{Haskell}
< ? 
  ├── > < ? 
  │       ├── > 0
  │       └── > 1
  └── > < ? 
          ├── > 1
          └── > 2
\end{minted}

In order to evaluate the example correctly, all choices need to have the same ID.
Since all scopes are copies of the same call to \hinl{share}, the sharing handler needs to behave equally for all scopes and the choices within.
However, this information is lost when the bind structure in the term duplicates the sharing scopes.
Hence, the \hinl{begin} and \hinl{end} tags of the scope receive an ID.
Although it would be sufficient to mark only the \hinl{begin} tags, it makes checking for mismatched tags easier to give \hinl{end} an ID, too.

\begin{minted}{Haskell}
data Share p = BShare' Int p | EShare' Int p
\end{minted}

With this new data type, how do we define the smart constructor \hinl{share}?
There are two options: \hinl{share} either receives an ID as a parameter or the ID is generated inside the function.
The former is much simpler to implement but would entail that the user needs to assign a unique ID to each call of \hinl{share}.
Since it is good practice to hide such implementation-specific details from the user, the second approach of generating an ID within \hinl{share} is the better option.

In order to generate an ID for a sharing scope, we need a state that the ID is derived from.
Again, we have two options.
The state could be implemented on the level of the \textit{modelling} language or the \textit{modelled} language.
The former would mean that all programs would need to be defined within the state monad, which is conceptually similar to the approach of user-defined IDs that are put into the program from the outside.

The latter approach uses the state effect on the \hinl{Prog} level, which was discussed in \autoref{sec:effectHandlers}.
This means that \hinl{share} itself becomes a complex program instead of a simple smart constructor.
In this case, the ID is generated within the program.

Generally, using the \hinl{Prog} state effect is preferable because it does not require adapting the whole infrastructure to the state monad and it ties in elegantly with the theme of modelling effects.

\begin{minted}{Haskell}
share :: (Share :<: sig, State Int :<: sig) 
      => Prog sig a -> Prog sig (Prog sig a)
share p = return $ do 
  i <- get
  put (i + 1)
  begin i
  x <- p
  end i
  return x
\end{minted}

The signature of the program now needs to support an integer state in order to support sharing syntax.
We still use \hinl{return} to create an empty, outer program layer.
The inner program now contains state syntax that retrieves and increments the current state.
The value from the state is then used as the ID of the sharing scope.

The consequence of the added state code is visualized by means of the initial example \hinl{addSharedCoin}.

\begin{minted}{Haskell}
do fx <- share coin
   addM fx fx
\end{minted}

Inlining the definition of \hinl{share} yields the following program.

\begin{minted}{Haskell}
do fx <- return $ do
     i <- get
     put (i + 1)
     begin i
     x <- coin
     end i
     return x
   addM fx fx -- state code is duplicated!
\end{minted}

Due to the left identity law for bind, \hinl{fx <- return $ ...} acts like a \hinl{let} binding where \hinl{fx} is bound to the program that follows \hinl{return}.
This results in the state code being duplicated in the addition.
Unfortunately, this is not the desired behavior, as the following visualization shows.

\begin{minted}[escapeinside=||]{Haskell}
<0 ? 
   ├── 0> <1 ? 
   │         ├── 1> 0
   │         └── 1> 1
   └── 0> <1 ? 
             ├── 1> 1
             └── 1> 2
\end{minted}

When the state is initialized with \hinl{0}, the first scope receives the ID \hinl{0} and increments the state to \hinl{1} when the state code within the first occurrence of \hinl{fx} is executed.
Then, the second \hinl{fx} is evaluated and the same happens again.
Thus, the ID of the following scope is \hinl{1} for both branches\footnote{In this example, the state handler runs before the non-determinism handler and thus, choice branches have a local state.}.
Since the idea of the added state is to link scopes together, so that duplicated scopes receive the same ID, this approach has failed.
Luckily, just a small modification is needed to fix the problem.
The problem of the current \hinl{share} implementation is that one part of the program -- the state code -- needs to be executed immediately, while the other part -- the shared program -- should only be evaluated if needed.
In the current implementation of \hinl{share}, there is an empty, outer program layer that is evaluated by bind when using \hinl{share}.
The reason for the nested program structure now becomes clear: The outer program layer contains the state code that is executed once when bind evaluates \hinl{share}.

\begin{minted}{Haskell}
do fx <- do -- state code is executed
     i <- get
     put (i + 1)
     return $ do
       begin i
       x <- coin
       end i
       return x
   addM fx fx
\end{minted}

Consequently, all occurrences of the state, that is, the scope IDs, are defined before the shared program is evaluated.
Thus, it does not matter if or where in the program the result of \hinl{share} is evaluated.
This is also reflected in the visualization of the example \hinl{addSharedCoin}.

\begin{minted}{Haskell}
<0 ? 
   ├── 0> <0 ? 
   │         ├── 0> 0
   │         └── 0> 1
   └── 0> <0 ? 
             ├── 0> 1
             └── 0> 2
\end{minted}

\subsection{Sharing Infrastructure}

\subsection{Nested Sharing}
With the current definition of the \hinl{share} operator, simple sharing examples are modelled correctly.
However, there are more complex scenarios that have not been considered yet.
For example, calls of \hinl{share} within a shared expression, that is, nested sharing, leads to incorrect behavior.
We consider the following example of adding the shared result of the addition of a shared coin.

\begin{minted}{Haskell}
exAddSharedCoinNested :: Prog (Share :+: ND) Int
exAddSharedCoinNested = share (share coini >>= \fx -> addM fx fx) >>= 
                          \fy -> addM fy fy
\end{minted}

The problematic part is generating the ID for the inner call of \hinl{share}.
Whereas the outer sharing scope correctly receives the ID \hinl{0} for both occurrences within the term structure, the ID of the inner scope differs.

\begin{minted}{Haskell}
<0 <1 ? 
      ├── 1> <1 ? 
      │         ├── 1> 0> <0 <2 ? 
      .         │               ├── 2> <2 ? 
      .         .               │         ├── 2> 0> 0
      .         .               .         └── 2> 0> 1
\end{minted}

The scope with ID \hinl{0} originate from the outer call of \hinl{share}, while the inner scopes correspond to the nested call.
Both scopes with the ID \hinl{0} should behave identically, including the nested scopes.
However, in the current implementation this is not the case.
When \hinl{fy} is evaluated for the first time, the inner call to \hinl{share} receives the ID \hinl{1}, since state was incremented by the first call.
The following scope with ID \hinl{0} is not affected by this because its ID was assigned together with the first scope.
The second nested scope is not linked to the first one, however, because the state code of both scopes is executed separately.
Thus, the increment operation from running the first nested \hinl{share} affects the second one and the ID \hinl{2} is assigned, although it should have been \hinl{1}.

The problem is this example is therefore that the nested \hinl{share} calls are duplicated but the state is not.
To solve this problem, we can add \hinl{put} to the program before \hinl{x <- p}, so that  nested calls of \hinl{share} in \hinl{p} behave identical if the scope program is duplicated. 

\begin{minted}{Haskell}
share :: (Share :<: sig, State Int :<: sig) 
      => Prog sig a -> Prog sig (Prog sig a)
share p = do
  i <- get
  put i + 1
  return $ do
    begin i
    put {- new state? -}
    x <- p
    end i
    return x
\end{minted}

For an example like \hinl{exAddSharedCoinNested}, the new state can be as simple as \hinl{i + 1}.
With the added \hinl{put} syntax, the state within the duplicated scope is no longer different to the state in the original scope.
Hence, the nested scope receives the correct ID.

\begin{minted}{Haskell}
<0 <1 ? 
      ├── 1> <1 ? 
      │         ├── 1> 0> <0 <1 ? 
      .         │               ├── 1> <1 ? 
      .         .               │         ├── 1> 0> 0
      .         .               .         └── 1> 0> 1
\end{minted}

This is not a universal solution, however, since ID clashes can occur in some situations.
When nested sharing is followed by another \hinl{share} call, as in the following example, the IDs inside the nested \hinl{share} and the IDs after the nested share can clash.

\begin{minted}{Haskell}
exAddSharedCoin4 :: Prog (Share :+: ND) Int
exAddSharedCoin4 =
  share (share coin >>= \fx -> addM fx fx) >>=
  \fy -> share coin >>= \fz -> addM fy fz
\end{minted}

The tree shows the scope \hinl{1} wrapped around the duplicated, nested \hinl{share} scopes with ID \hinl{2}.
After that, another scope with ID \hinl{2} follows, although this scope belongs to the shared coin \hinl{fz}.

\begin{minted}{Haskell}
<1 <2 ? 
      ├── 2> <2 ? 
      │         ├── 2> 1> <2 ?
      .         │            ├── 2> 0
      .         .            └── 2> 1
\end{minted}

This clash occurred because nested sharing and repeated sharing have the same namespace when \hinl{put (i + 1)} is used to set the scope state.
In order to make the namespaces unique, one option is to have \hinl{put (i * 2)} in the outer program layer and \hinl{put (i * 2 + 1)} for the inner program layer.
In the adapted syntax tree we can now see that the nested calls have the ID \hinl{2 * 1 + 1 = 3}, while the repeated call received the ID \hinl{2 * 1 = 2}.
Most importantly, the IDs of the nested scopes and the last scope are different now.

\begin{minted}{Haskell}
<1 <3 ? 
      ├── 3> <3 ? 
      │         ├── 3> 1> <2 ? 
      .         │            ├── 2> 0
      .         .            └── 2> 1

\end{minted}

The \hinl{*2/*2+1} approach is used, for example, in the KiCS2 compiler.
It can lead to large numbers very quickly, however, and is not suitable for Coq due to its Peano representation of numbers.
A more elegant solution can be implemented using a pair of integers as state.
This way, one component is incremented in the outer program layer and the other component in the inner layer.

\subsection{Deep Sharing}

\chapter{Call-Time Choice modelled in Coq}
\label{ch:callTimeChoiceCoq}
The goal of this chapter is to transfer the Haskell implementation of call-time choice to Coq.
We begin with the data structure \hinl{Prog}, that is, the free monad, which allowed us to model programs with effects of type \hinl{sig} and results of type \hinl{a}.

\begin{minted}{Haskell}
data Prog sig a = Return a | Op (sig (Prog sig a))
\end{minted}

The definition in Coq looks very similar to the Haskell version, aside from renaming and the explicit constructor types.

\begin{minted}{Coq}
Inductive Free F A :=
| pure : A -> Free F A
| impure : F (Free F A) -> Free F A.
\end{minted}

However, the definition is rejected by Coq upon loading the file with the following error message.

\begin{minted}{Text}
Non-strictly positive occurrence of "Free" in "F (Free F A) -> Free F A".
\end{minted}

The reason for this error is explained in the next section.

\section{Non-strictly Positive Occurrence}
\begin{itemize}
\item What does non-strictly positive occurrence mean?
\item Motivation for usage of containers
\end{itemize}

In \autoref{sec:coqIntro}, we learned that Coq distinguishes between non-recursive definitions and functions that use recursion.
The reason for this is that Coq checks functions for termination, which is an important part of Coq's proof logic.
To understand why functions must always terminate in Coq, we consider the following function.

% https://www.di.ens.fr/~zappa/teaching/coq/ecole11/summer/lectures/lec9.pdf
\begin{minted}{Coq}
Fail Fixpoint loop (x : unit) : A := loop x.
\end{minted}

The function receives an argument \mintinline{Coq}{x} and calls itself with the same argument.
Since this function obviously never terminates, the result type \mintinline{Coq}{A} is arbitrary.
In particular, we could instantiate \mintinline{Coq}{A} with \mintinline{Coq}{False}, the false proposition.
The value \mintinline{Coq}{loop tt : False} could be used to prove anything, according to the principle of explosion.
For this reason, Coq requires all recursive functions to terminate provably.

Returning to the original data type, what is link between \mintinline{Coq}{Free} and termination?
It is well known that recursion can be implemented in languages without explicit recursion syntax by means of constructs like the Y combinator or the data type \mintinline{Coq}{Mu} for type-level recursion.

\begin{minted}{Coq}
Fail Inductive Mu A := mu : (Mu A -> A) -> Mu A.
\end{minted}

\mintinline{Coq}{Mu} is not accepted by Coq for the same reason as \mintinline{Coq}{Free}: non-strictly positive occurrence of the respective data type.
The problematic property of non-strictly positive data types is that the type occurs on the left-hand side of a constructor argument's function type.
This would allow general recursion and thus, as described above, make Coq's logic inconsistent.

In case of \mintinline{Coq}{Free}, the non-strictly positive occurrence is not as apparent as before because the constructors do not have functional arguments.
However, \mintinline{Coq}{F} is being applied to \mintinline{Coq}{Free F A}.
If \mintinline{Coq}{F} has a functional argument with appropriate types, the resulting type becomes non-strictly positive, as shown below.

\begin{minted}{Coq}
Definition Cont R A := (A -> R) -> R.

(* Free (Cont R) *)
Fail Inductive ContF R A :=
| pureC   : A -> ContF R A
| impureC : ((ContF R A -> R) -> R) -> ContF R A.
\end{minted}

In the type of \mintinline{Coq}{impureC} contains a non-strictly positive occurrence of \mintinline{Coq}{ContF R A}.
Consequently, Coq rejects \mintinline{Coq}{Free} because it is not guaranteed that no instance violates the strict positivity requirement.
Representing the \mintinline{Coq}{Free} data type therefore requires a way to restrict the definition to strictly positive data types.
One approach to achieve this goal is described in the next section.

\section{Containers}

\begin{itemize}
\item How do containers work?
\item How do we translate effect functors into containers?
\end{itemize}

Containers are an abstraction of data types that store values, with the property that only strictly positive data types can be modelled as a container.
This will allow us to define a version of \mintinline{Coq}{Free} that works with containers of type constructors instead of the type constructors itself.
First, however, we will have a more detailed look at containers.

The first component of a container is the type \mintinline{Coq}{Shape}.
A shape determines how the data type is structured, regardless of the stored values.
For example, the  shape of a list is the same as the shape of Peano numbers: a number that  represents the length of the list, or rather the number of \hinl{Cons}/\hinl{Succ} applications.
A pair, on the other hand, has only a single shape.

The second component of a container is a function \mintinline{Coq}{Pos : Shape -> Type} that gives each shape a type that represents the positions within the shape.
In the example of pairs, the shape has two positions, the first and second component.
Each element of a list is a position within the shape.
Therefore, the position type for lists with length $n$ is natural numbers smaller than $n$.
Peano numbers do not have elements and therefore, the position type for each shape is empty.

Containers can be extended by a function that maps all valid positions to values.
Since the position type depends on a concrete shape, the definition in Coq is quantified universally over values of type \mintinline{Coq}{Shape}.

\begin{minted}{Coq}
Inductive Ext Shape (Pos : Shape -> Type) A := 
  ext : forall s, (Pos s -> A) -> Ext Shape Pos A.
\end{minted}

The extension of a container models the concrete data type.

\section{Modelling Effects}
\begin{itemize}
\item In which ways is the Coq implementation simplified, compared to Haskell?
\item How does the adapted Prog/sig infrastructure work?
\item How do we translate recursive functions?
\end{itemize}

\section{Sharing}
\label{sec:lawsOfSharing}
\begin{itemize}
\item Laws of sharing
\end{itemize}

\chapter{Curry Programs modelled in Coq}
\begin{itemize}
\item Can we use the Coq model of call-time choice to prove properties about actual Curry programs?
\end{itemize}

\chapter{Conclusion}

\begin{minted}{Coq}

  Class Monad (M: Type → Type) ≔
    {
      ret : ∀ A, A → M A;
      bind : ∀ A B, M A → (A → M B) → M B;
      left_unit : ∀ A B (x0: A) (f: A → M B),
                    bind (ret x0) f = f x0;
      right_unit : ∀ A (ma: M A), bind ma (@ret A) = ma;
      bind_assoc: ∀ A B C (ma : M A) (f: A → M B) (g: B → M C),
                    bind ma (fun y ⇒ bind (f y) g) = bind (bind ma f) g
    }.

  Definition join (M: Type → Type) `(Monad M) A (mmx : M (M A)) : M A ≔ bind _ mmx (fun x ⇒ x).

End MonadClass.
Arguments join {_} {_} {_}.

Section MonadInstance.

  Variable F : Type → Type.
  Variable C__F : Container F.


\end{minted}


% Literatur
\bibliographystyle{plainnat}
\bibliography{Mathesis} % Datei: seminar.bib
% Anhang
\appendix
\end{document}